{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Topic Modelling vs LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import boto3\n",
    "import shutil\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import sagemaker.amazon.common as smac\n",
    "from pprint import pprint\n",
    "from sagemaker.session import s3_input\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4370k  100 4370k    0     0  18.4M      0 --:--:-- --:--:-- --:--:-- 18.4M\n",
      "Archive:  wikitext-2-v1.zip\n",
      "replace datasets/wikitext-2/wiki.test.tokens? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "#Wiki datasets\n",
    "!curl -O https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
    "!unzip wikitext-2-v1.zip -d datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_document_start(line):\n",
    "    if len(line) < 4:\n",
    "        return False\n",
    "    if line[0] is '=' and line[-1] is '=':\n",
    "        if line[2] is not '=':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def token_list_per_doc(input_dir, token_file):\n",
    "    lines_list = []\n",
    "    line_prev = ''\n",
    "    prev_line_start_doc = False\n",
    "    with open(os.path.join(input_dir, token_file), 'r', encoding='utf-8') as f:\n",
    "        for l in f:\n",
    "            line = l.strip()\n",
    "            if prev_line_start_doc and line:\n",
    "                # the previous line should not have been start of a document!\n",
    "                lines_list.pop()\n",
    "                lines_list[-1] = lines_list[-1] + ' ' + line_prev\n",
    "\n",
    "            if line:\n",
    "                if is_document_start(line) and not line_prev:\n",
    "                    lines_list.append(line)\n",
    "                    prev_line_start_doc = True\n",
    "                else:\n",
    "                    lines_list[-1] = lines_list[-1] + ' ' + line\n",
    "                    prev_line_start_doc = False\n",
    "            else:\n",
    "                prev_line_start_doc = False\n",
    "            line_prev = line\n",
    "\n",
    "    print(\"{} documents parsed!\".format(len(lines_list)))\n",
    "    return lines_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 documents parsed!\n",
      "60 documents parsed!\n",
      "60 documents parsed!\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'datasets/wikitext-2'\n",
    "train_file = 'wiki.train.tokens'\n",
    "val_file = 'wiki.valid.tokens'\n",
    "test_file = 'wiki.test.tokens'\n",
    "\n",
    "train_doc_list = token_list_per_doc(input_dir, train_file)\n",
    "val_doc_list = token_list_per_doc(input_dir, val_file)\n",
    "test_doc_list = token_list_per_doc(input_dir, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sanitisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(doc):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "    return [\n",
    "        wnl.lemmatize(word) \n",
    "        for word in doc.split() \n",
    "        if len(word) >= 2 \n",
    "        and re.match(\"[a-z].*\", word)         \n",
    "        and re.match(token_pattern, word)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function lemmatize at 0x7fdff59a3bf8>, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    input='content', \n",
    "    analyzer='word', #\n",
    "    stop_words='english', #remove words that add no value\n",
    "    tokenizer=lemmatize, #pass function reference to lemmatizer\n",
    "    \n",
    "    #Consider tuning these two to your specific use case\n",
    "    max_df=0.90, #Remove terms that appear in more than 90% of documents\n",
    "    min_df=3 #Remove words that appear in less than 3 documents\n",
    ")\n",
    "\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Apply the lemmatisation / vectorisation\n",
    "\n",
    "#fit_transform learns the vocab dictionary of all tokens, and returns the term-document matrix\n",
    "#transform simply returns the term-document matrix\n",
    "train_vectors = vectorizer.fit_transform(train_doc_list)\n",
    "\n",
    "test_vectors = vectorizer.transform(test_doc_list)\n",
    "val_vectors = vectorizer.transform(val_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16650"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = vectorizer.get_feature_names()\n",
    "vocab_size = len(vocab_list)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the matrices\n",
    "def shuffle_csr_matrix(matrix):\n",
    "    indices = np.arange(matrix.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    matrix = matrix[indices]\n",
    "    return matrix\n",
    "    \n",
    "\n",
    "train_vectors = shuffle_csr_matrix(train_vectors)\n",
    "test_vectors = shuffle_csr_matrix(test_vectors)\n",
    "val_vectors = shuffle_csr_matrix(val_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTM expects float32 inputs\n",
    "#Convert the entries in each matrix to float32\n",
    "train_vectors = sparse.csr_matrix(train_vectors, dtype=np.float32)\n",
    "test_vectors = sparse.csr_matrix(test_vectors, dtype=np.float32)\n",
    "val_vectors = sparse.csr_matrix(val_vectors, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data to RecordIO protobuf\n",
    "def convert_to_protobuf(sparray, prefix):\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(array=sparray[:], file=buf, labels=None)\n",
    "    buf.seek(0)\n",
    "\n",
    "    #fname = os.path.join(prefix, fname_template.format(i))\n",
    "    with open(f'data/{prefix}_data', 'wb') as f:\n",
    "        f.write(buf.getvalue())\n",
    "    print('Saved data to {}'.format(f'data/{prefix}_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to data/train_data\n",
      "Saved data to data/test_data\n",
      "Saved data to data/validation_data\n"
     ]
    }
   ],
   "source": [
    "convert_to_protobuf(train_vectors, prefix='train')\n",
    "convert_to_protobuf(test_vectors, prefix='test')\n",
    "convert_to_protobuf(val_vectors, prefix='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.5M\r\n",
      "drwxrwxr-x  2 ec2-user ec2-user 4.0K Feb 19 23:23 .\r\n",
      "drwxrwxrwx 11 ec2-user ec2-user 4.0K Mar  2 01:09 ..\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user 213K Mar  2 01:10 test_data\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user 2.1M Mar  2 01:10 train_data\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user 205K Mar  2 01:10 validation_data\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the vocab auxilliary file\n",
    "!mkdir -p auxilliary\n",
    "with open('auxilliary/vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in vocab_list:\n",
    "        f.write(item+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the data files and vocab file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "BucketAlreadyOwnedByYou",
     "evalue": "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f81fd7b811b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m boto3.client('s3', region_name=region).create_bucket(\n\u001b[1;32m      6\u001b[0m     \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mCreateBucketConfiguration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'LocationConstraint'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m: An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it."
     ]
    }
   ],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "region = boto3.session.Session().region_name\n",
    "bucket_name = f\"neural-topic-modelling-{account_id}\" #Generate a unique bucket name\n",
    "\n",
    "boto3.client('s3', region_name=region).create_bucket(\n",
    "    Bucket=bucket_name, \n",
    "    CreateBucketConfiguration={'LocationConstraint': region}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/train_data to s3://neural-topic-modelling-817322365495/train/train_data\n",
      "upload: data/test_data to s3://neural-topic-modelling-817322365495/test/test_data\n",
      "upload: data/validation_data to s3://neural-topic-modelling-817322365495/validation/validation_data\n",
      "upload: auxilliary/vocab.txt to s3://neural-topic-modelling-817322365495/auxilliary/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "#Copy data to s3\n",
    "s3_training_data_loc = f's3://{bucket_name}/train/train_data'\n",
    "s3_testing_data_loc = f's3://{bucket_name}/test/test_data'\n",
    "s3_validation_data_loc = f's3://{bucket_name}/validation/validation_data'\n",
    "s3_vocab_data_loc = f's3://{bucket_name}/auxilliary/vocab.txt'\n",
    "\n",
    "\n",
    "!aws s3 cp data/train_data s3://$bucket_name/train/train_data\n",
    "!aws s3 cp data/test_data s3://$bucket_name/test/test_data\n",
    "!aws s3 cp data/validation_data s3://$bucket_name/validation/validation_data\n",
    "!aws s3 cp auxilliary/vocab.txt s3://$bucket_name/auxilliary/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')\n",
    "ntm = sagemaker.estimator.Estimator(\n",
    "    container, #Use the pre-built NTM container\n",
    "    role,\n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.c4.xlarge', #'ml.p3.2xlarge', \n",
    "    output_path=f's3://{bucket_name}/models/model',\n",
    "    sagemaker_session=session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "ntm.set_hyperparameters(num_topics=num_topics, feature_dim=vocab_size, mini_batch_size=60, \n",
    "                        epochs=50, sub_sample=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train = s3_input(s3_training_data_loc, distribution='ShardedByS3Key',\n",
    "                    content_type='application/x-recordio-protobuf')\n",
    "s3_test = s3_input(s3_testing_data_loc, distribution='FullyReplicated',\n",
    "                  content_type='application/x-recordio-protobuf')\n",
    "s3_val = s3_input(s3_validation_data_loc, distribution='FullyReplicated',\n",
    "                  content_type='application/x-recordio-protobuf')\n",
    "s3_vocab = s3_input(s3_vocab_data_loc, distribution='FullyReplicated', content_type='text/plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-02 01:14:53 Starting - Starting the training job...\n",
      "2020-03-02 01:14:54 Starting - Launching requested ML instances......\n",
      "2020-03-02 01:16:18 Starting - Preparing the instances for training......\n",
      "2020-03-02 01:16:55 Downloading - Downloading input data...\n",
      "2020-03-02 01:17:52 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:17:54 INFO 140117196126016] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:17:54 INFO 140117196126016] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'50', u'feature_dim': u'16650', u'mini_batch_size': u'60', u'num_topics': u'20', u'sub_sample': u'0.7'}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:17:54 INFO 140117196126016] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'16650', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'3', u'epochs': u'50', u'mini_batch_size': u'60', u'num_topics': u'20', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'0.7', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:17:54 INFO 140117196126016] nvidia-smi took: 0.0251739025116 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:17:54 INFO 140117196126016] Using default worker.\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:17:54.691] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:17:54.694] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:17:54 INFO 140117196126016] Initializing\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:17:54 INFO 140117196126016] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:17:54 INFO 140117196126016] vocab.txt\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:17:54 INFO 140117196126016] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:17:54 INFO 140117196126016] Loading pre-trained token embedding vectors from /opt/amazon/lib/python2.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\n",
      "2020-03-02 01:18:24 Uploading - Uploading generated training model\n",
      "2020-03-02 01:18:24 Completed - Training job completed\n",
      "\u001b[34m[03/02/2020 01:18:09 WARNING 140117196126016] 70 out of 16650 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:09 INFO 140117196126016] Vocab embedding shape\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1583111890.040368, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1583111890.040309}\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:10.040] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 15353, \"num_examples\": 1, \"num_bytes\": 226364}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] \u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:10.668] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 623, \"num_examples\": 10, \"num_bytes\": 2101536}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] # Finished training epoch 1 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Loss (name: value) total: 9.44348068237\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Loss (name: value) kld: 0.027827325634\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Loss (name: value) recons: 9.41565322876\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Loss (name: value) logppx: 9.44348068237\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=9.44348068237\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:10.674] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 15982, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:10.699] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 25, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Loss (name: value) total: 9.00726013184\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Loss (name: value) kld: 0.0225669840972\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Loss (name: value) recons: 8.98469340007\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Loss (name: value) logppx: 9.00726013184\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] #validation_score (1): 9.007260131835938\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] Timing: train: 0.63s, val: 0.03s, epoch: 0.66s\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Total Records Seen\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1583111890.702171, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1583111890.040751}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] #throughput_metric: host=algo-1, train throughput=906.905953707 records/second\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] \u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:10 INFO 140117196126016] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:11.299] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 596, \"num_examples\": 10, \"num_bytes\": 2101536}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] # Finished training epoch 2 on 540 examples from 9 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Subsampled 9 batches out of 10 total batches.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) total: 8.82837433992\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) kld: 0.0583036846585\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) recons: 8.77007073296\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) logppx: 8.82837433992\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=8.82837433992\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:11.324] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) total: 8.59949544271\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) kld: 0.0723103602727\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) recons: 8.52718454997\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) logppx: 8.59949544271\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] #validation_score (2): 8.599495442708333\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Timing: train: 0.60s, val: 0.03s, epoch: 0.63s\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Total Batches Seen\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"Total Records Seen\": {\"count\": 1, \"max\": 1200, \"sum\": 1200.0, \"min\": 1200}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1583111891.33038, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1583111890.702472}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] #throughput_metric: host=algo-1, train throughput=955.286320285 records/second\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] \u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:11.787] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 455, \"num_examples\": 10, \"num_bytes\": 2101536}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] # Finished training epoch 3 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) total: 8.69055771601\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) kld: 0.0582912899199\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) recons: 8.63226667132\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) logppx: 8.69055771601\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=8.69055771601\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:11.812] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) total: 8.58863830566\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) kld: 0.0381311456362\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) recons: 8.5505065918\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Loss (name: value) logppx: 8.58863830566\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] #validation_score (3): 8.588638305664062\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] Timing: train: 0.46s, val: 0.03s, epoch: 0.49s\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Total Batches Seen\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}, \"Total Records Seen\": {\"count\": 1, \"max\": 1800, \"sum\": 1800.0, \"min\": 1800}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1583111891.81807, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1583111891.330735}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] #throughput_metric: host=algo-1, train throughput=1230.6778654 records/second\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] \u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:11 INFO 140117196126016] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:12.336] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 517, \"num_examples\": 10, \"num_bytes\": 2101536}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] # Finished training epoch 4 on 480 examples from 8 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Subsampled 8 batches out of 10 total batches.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) total: 8.6574567159\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) kld: 0.0538153121869\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) recons: 8.60364163717\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) logppx: 8.6574567159\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=8.6574567159\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:12.366] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 29, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) total: 8.55550333659\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) kld: 0.040509557724\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) recons: 8.51499328613\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) logppx: 8.55550333659\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] #validation_score (4): 8.555503336588542\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] patience losses:[9.007260131835938, 8.599495442708333, 8.588638305664062] min patience loss:8.58863830566 current loss:8.55550333659 absolute loss difference:0.0331349690755\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Timing: train: 0.52s, val: 0.03s, epoch: 0.55s\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Total Batches Seen\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}, \"Total Records Seen\": {\"count\": 1, \"max\": 2400, \"sum\": 2400.0, \"min\": 2400}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1583111892.371634, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1583111891.818445}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] #throughput_metric: host=algo-1, train throughput=1084.31455597 records/second\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] \u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:12.764] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 391, \"num_examples\": 10, \"num_bytes\": 2101536}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] # Finished training epoch 5 on 360 examples from 6 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Subsampled 6 batches out of 10 total batches.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) total: 8.61179182265\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) kld: 0.0496291710271\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) recons: 8.56216244168\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) logppx: 8.61179182265\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=8.61179182265\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:12.789] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) total: 8.53985697428\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) kld: 0.0414172212283\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) recons: 8.49843953451\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Loss (name: value) logppx: 8.53985697428\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] #validation_score (5): 8.539856974283854\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] patience losses:[8.599495442708333, 8.588638305664062, 8.555503336588542] min patience loss:8.55550333659 current loss:8.53985697428 absolute loss difference:0.0156463623047\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] Timing: train: 0.39s, val: 0.03s, epoch: 0.42s\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Total Batches Seen\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}, \"Total Records Seen\": {\"count\": 1, \"max\": 3000, \"sum\": 3000.0, \"min\": 3000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1583111892.793711, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1583111892.371896}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] #throughput_metric: host=algo-1, train throughput=1421.96587055 records/second\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] \u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:12 INFO 140117196126016] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:13.142] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 346, \"num_examples\": 10, \"num_bytes\": 2101536}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] # Finished training epoch 6 on 300 examples from 5 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Subsampled 5 batches out of 10 total batches.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) total: 8.60329447428\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) kld: 0.0482276837031\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) recons: 8.5550664266\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) logppx: 8.60329447428\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=8.60329447428\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:13.167] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) total: 8.53209940592\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) kld: 0.0370248794556\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) recons: 8.49507446289\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) logppx: 8.53209940592\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] #validation_score (6): 8.532099405924479\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] patience losses:[8.588638305664062, 8.555503336588542, 8.539856974283854] min patience loss:8.53985697428 current loss:8.53209940592 absolute loss difference:0.00775756835938\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Timing: train: 0.35s, val: 0.03s, epoch: 0.38s\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Total Batches Seen\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}, \"Total Records Seen\": {\"count\": 1, \"max\": 3600, \"sum\": 3600.0, \"min\": 3600}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1583111893.171841, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1583111892.793959}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] #throughput_metric: host=algo-1, train throughput=1587.23442366 records/second\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] \u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:13.700] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 526, \"num_examples\": 10, \"num_bytes\": 2101536}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] # Finished training epoch 7 on 360 examples from 6 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Subsampled 6 batches out of 10 total batches.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) total: 8.59839324951\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) kld: 0.0479494598177\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) recons: 8.55044403076\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) logppx: 8.59839324951\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=8.59839324951\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:13.738] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 32, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) total: 8.60663248698\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) kld: 0.0279006282489\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) recons: 8.5787322998\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Loss (name: value) logppx: 8.60663248698\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] #validation_score (7): 8.606632486979167\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] patience losses:[8.555503336588542, 8.539856974283854, 8.532099405924479] min patience loss:8.53209940592 current loss:8.60663248698 absolute loss difference:0.0745330810547\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] Timing: train: 0.53s, val: 0.04s, epoch: 0.57s\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Total Batches Seen\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}, \"Total Records Seen\": {\"count\": 1, \"max\": 4200, \"sum\": 4200.0, \"min\": 4200}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1583111893.739325, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1583111893.172086}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] #throughput_metric: host=algo-1, train throughput=1057.44176802 records/second\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] \u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:13 INFO 140117196126016] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:14.267] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 527, \"num_examples\": 10, \"num_bytes\": 2101536}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] # Finished training epoch 8 on 360 examples from 6 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Subsampled 6 batches out of 10 total batches.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) total: 8.59653354221\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) kld: 0.0443812476264\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) recons: 8.55215208266\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) logppx: 8.59653354221\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=8.59653354221\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:14.292] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) total: 8.56999104818\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) kld: 0.0305056730906\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) recons: 8.53948465983\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) logppx: 8.56999104818\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] #validation_score (8): 8.569991048177084\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] patience losses:[8.539856974283854, 8.532099405924479, 8.606632486979167] min patience loss:8.53209940592 current loss:8.56999104818 absolute loss difference:0.0378916422526\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Timing: train: 0.53s, val: 0.02s, epoch: 0.55s\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Total Batches Seen\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}, \"Total Records Seen\": {\"count\": 1, \"max\": 4800, \"sum\": 4800.0, \"min\": 4800}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1583111894.293805, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1583111893.739613}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] #throughput_metric: host=algo-1, train throughput=1082.3740311 records/second\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] \u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:14.835] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 541, \"num_examples\": 10, \"num_bytes\": 2101536}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] # Finished training epoch 9 on 420 examples from 7 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Subsampled 7 batches out of 10 total batches.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) total: 8.58516656785\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) kld: 0.042593673865\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) recons: 8.5425731114\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) logppx: 8.58516656785\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=8.58516656785\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:14.873] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 35, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) total: 8.53151448568\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) kld: 0.046114552021\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) recons: 8.485399882\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Loss (name: value) logppx: 8.53151448568\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] #validation_score (9): 8.531514485677084\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] patience losses:[8.532099405924479, 8.606632486979167, 8.569991048177084] min patience loss:8.53209940592 current loss:8.53151448568 absolute loss difference:0.000584920247395\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] Timing: train: 0.54s, val: 0.04s, epoch: 0.59s\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Total Batches Seen\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}, \"Total Records Seen\": {\"count\": 1, \"max\": 5400, \"sum\": 5400.0, \"min\": 5400}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1583111894.880261, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1583111894.294061}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] #throughput_metric: host=algo-1, train throughput=1022.73572951 records/second\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] \u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:14 INFO 140117196126016] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:15.325] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 443, \"num_examples\": 10, \"num_bytes\": 2101536}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] # Finished training epoch 10 on 300 examples from 5 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Subsampled 5 batches out of 10 total batches.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) total: 8.60956034342\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) kld: 0.0425277042389\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) recons: 8.56703226725\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) logppx: 8.60956034342\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=8.60956034342\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:15.370] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 29, \"duration\": 40, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) total: 8.53258666992\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) kld: 0.0309195359548\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) recons: 8.50166676839\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) logppx: 8.53258666992\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] #validation_score (10): 8.532586669921875\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] patience losses:[8.606632486979167, 8.569991048177084, 8.531514485677084] min patience loss:8.53151448568 current loss:8.53258666992 absolute loss difference:0.00107218424479\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Timing: train: 0.45s, val: 0.04s, epoch: 0.49s\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Total Batches Seen\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"Total Records Seen\": {\"count\": 1, \"max\": 6000, \"sum\": 6000.0, \"min\": 6000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1583111895.37202, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1583111894.881109}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] #throughput_metric: host=algo-1, train throughput=1221.78428555 records/second\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 WARNING 140117196126016] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:15.435] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 32, \"duration\": 44, \"num_examples\": 1, \"num_bytes\": 209496}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) total: 8.63256734212\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) kld: 0.0227539300919\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) recons: 8.60981343587\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) logppx: 8.63256734212\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] #quality_metric: host=algo-1, epoch=10, validation total_loss <loss>=8.63256734212\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss of server-side model: 8.63256734212\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Best model based on early stopping at epoch 9. Best loss: 8.53151448568\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Topics from epoch:final (num_topics:20) [wetc 0.38, tu 0.60]:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.32, 0.53] female year service specie total like american wale life lake line record wind final later km type king including sr\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.40, 0.52] series final specie book city season character team year road episode single appearance called member company king river black video\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.35, 0.60] depression series hurricane sr city construction blood soon time group caused american ship released rating gun foot ha album australia\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.34, 0.68] state sr royal burn sale road british designated school cruiser wild cyclone year highest market day national play crew written\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.37, 0.68] u film soundtrack km august year album east million form body central guitar meaning southeast north dock goal cross unit\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.47, 0.63] award september game began member best area mexico australian released second united included force region no. local traffic television chart\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.50, 0.62] music known art played writing time second home york building event number october final state blue called island make sea\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.22, 0.79] private effect johnson poem form army said stage kg ha image freeway election claim wrote blood bright sr wale yankovic\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.38, 0.58] love play john september south university award shot people cyclone tropical film wrote white track road park able canadian used\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.33, 0.59] story fan km community loss tropical german force road storm god wind battle state carey ship love title burn average\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.45, 0.56] used ha time ship island song world star cyclone day number originally early state run lost large route national old\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.26, 0.76] cm damage intensity storm northwest estimated mi utc sister wood average region max university activity hour sea kingdom specie stem\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.43, 0.55] australian team battalion land season end time white final ha week early state troop yard term original player united song\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.37, 0.59] st ha album player cross season north film california playing game eastern november line selected debut award battalion share sea\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.44, 0.64] team school area intersection south lake vehicle use no. including section film car story following ball major ny began east\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.38, 0.49] ship ha white new ft book century episode road band series according known south public including city life place specie\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.44, 0.46] new time near song tropical year number wind u known used york designed highway eastern september king late game hurricane\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.36, 0.56] storm season series final damage general hurricane million release style new ha week era army university church known artist pitch\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.42, 0.56] storm game year hurricane foot service book young colony study body long area star england character hill battle eastern recorded\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] [0.37, 0.58] story played final performance hill old critic wood april later win sr south character won ny event band area color\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Saved checkpoint to \"/tmp/tmpchmiED/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:15.628] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 20934, \"num_examples\": 1, \"num_bytes\": 217208}\u001b[0m\n",
      "\u001b[34m[2020-03-02 01:18:15.671] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 43, \"num_examples\": 1, \"num_bytes\": 217208}\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Finished scoring on 60 examples from 1 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) total: 8.57893269857\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) kld: 0.0538435022036\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) recons: 8.5250890096\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] Loss (name: value) logppx: 8.57893269857\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1583111895.672717, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1583111895.628589}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/02/2020 01:18:15 INFO 140117196126016] #test_score (algo-1) : ('log_perplexity', 8.578932698567709)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 21053.81202697754, \"sum\": 21053.81202697754, \"min\": 21053.81202697754}, \"finalize.time\": {\"count\": 1, \"max\": 237.8401756286621, \"sum\": 237.8401756286621, \"min\": 237.8401756286621}, \"initialize.time\": {\"count\": 1, \"max\": 15343.101024627686, \"sum\": 15343.101024627686, \"min\": 15343.101024627686}, \"model.serialize.time\": {\"count\": 1, \"max\": 18.09406280517578, \"sum\": 18.09406280517578, \"min\": 18.09406280517578}, \"setuptime\": {\"count\": 1, \"max\": 38.18702697753906, \"sum\": 38.18702697753906, \"min\": 38.18702697753906}, \"early_stop.time\": {\"count\": 10, \"max\": 43.370962142944336, \"sum\": 322.5076198577881, \"min\": 24.884939193725586}, \"update.time\": {\"count\": 10, \"max\": 661.1661911010742, \"sum\": 5325.979232788086, \"min\": 377.7289390563965}, \"epochs\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}, \"model.score.time\": {\"count\": 12, \"max\": 53.27105522155762, \"sum\": 390.7747268676758, \"min\": 24.441957473754883}}, \"EndTime\": 1583111895.674247, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1583111874.686206}\n",
      "\u001b[0m\n",
      "Training seconds: 89\n",
      "Billable seconds: 89\n"
     ]
    }
   ],
   "source": [
    "model = ntm.fit({'train': s3_train, 'validation': s3_val, 'auxiliary': s3_vocab, 'test': s3_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: ntm-2020-03-02-01-14-53-592\n"
     ]
    }
   ],
   "source": [
    "print('Training job name: {}'.format(ntm.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "predictor = ntm.deploy(\n",
    "    instance_type='ml.m5.xlarge', \n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=\"neural-topic-modelling\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.content_type = 'text/csv'\n",
    "predictor.serializer = csv_serializer\n",
    "predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predictor.predict(\n",
    "    val_vectors.getrow(0).toarray()[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04545095, 0.09137032, 0.02837488, 0.0354532 , 0.01577244,\n",
       "        0.06526145, 0.08169498, 0.01276142, 0.05082734, 0.0359521 ,\n",
       "        0.09134295, 0.00839241, 0.0545993 , 0.0449562 , 0.04433573,\n",
       "        0.08488413, 0.10063872, 0.02832018, 0.04672164, 0.03288965]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each entry in this array represents a computed topic\n",
    "#note that len(predictions) = num_topics\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
